{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDr-usY9p-V7"
      },
      "source": [
        "## English to Indonesian translation using attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pIR6m0HWp-V8",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oeRMKk0hp-V_",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6BwzOdjOp-WB",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1u_5JohsvaFHiQ8r_YnL34aA66sSTHfHX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obAmz_dmvoF2",
        "outputId": "380281fe-6a60-410d-ff4e-a69f3effb410"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1u_5JohsvaFHiQ8r_YnL34aA66sSTHfHX\n",
            "To: /content/eng-indo-augmented.txt\n",
            "\r  0% 0.00/1.14M [00:00<?, ?B/s]\r100% 1.14M/1.14M [00:00<00:00, 158MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "P6eOyoJEp-WE",
        "outputId": "c66de632-3d71-4acc-f31f-94bbfd50f090",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15531, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  English Indonesian\n",
              "0   run !     lari !\n",
              "1   who ?    siapa ?\n",
              "2   wow !      wow !\n",
              "3  help !   tolong !\n",
              "4  jump !   lompat !"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5276220d-5258-4403-8383-e9601013c19b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Indonesian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>run !</td>\n",
              "      <td>lari !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>who ?</td>\n",
              "      <td>siapa ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>wow !</td>\n",
              "      <td>wow !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>help !</td>\n",
              "      <td>tolong !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jump !</td>\n",
              "      <td>lompat !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5276220d-5258-4403-8383-e9601013c19b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5276220d-5258-4403-8383-e9601013c19b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5276220d-5258-4403-8383-e9601013c19b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a0786f14-affe-4a8c-a933-bc4b93b97792\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0786f14-affe-4a8c-a933-bc4b93b97792')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a0786f14-affe-4a8c-a933-bc4b93b97792 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "fp = open('./eng-indo-augmented.txt', 'r')\n",
        "text = fp.read()\n",
        "text = text.splitlines()\n",
        "fp.close()\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "text_dict = {\"English\": [], \"Indonesian\": []}\n",
        "for l in text:\n",
        "    split_text = l.split(\"\\t\")\n",
        "    text_dict[\"English\"].append(normalizeString(split_text[0]))\n",
        "    text_dict[\"Indonesian\"].append(normalizeString(split_text[1]))\n",
        "\n",
        "df = pd.DataFrame.from_dict(text_dict)\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eGVYxutbp-WG",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 25\n",
        "MIN_LENGTH = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "i0YKRNLpp-WI",
        "outputId": "d8b87f08-8e9d-42e3-8090-fc33abfe349b",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15531, 3)\n",
            "Current shape: (15531, 3)\n",
            "New shape: (15526, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index English Indonesian\n",
              "0      0   run !     lari !\n",
              "1      1   who ?    siapa ?\n",
              "2      2   wow !      wow !\n",
              "3      3  help !   tolong !\n",
              "4      4  jump !   lompat !"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cd868174-e528-4ffb-beaa-7a9d941f92f9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>English</th>\n",
              "      <th>Indonesian</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>run !</td>\n",
              "      <td>lari !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>who ?</td>\n",
              "      <td>siapa ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>wow !</td>\n",
              "      <td>wow !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>help !</td>\n",
              "      <td>tolong !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>jump !</td>\n",
              "      <td>lompat !</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd868174-e528-4ffb-beaa-7a9d941f92f9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cd868174-e528-4ffb-beaa-7a9d941f92f9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cd868174-e528-4ffb-beaa-7a9d941f92f9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cc31bfc7-cf96-44c8-a9f2-d58ad1596eae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc31bfc7-cf96-44c8-a9f2-d58ad1596eae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cc31bfc7-cf96-44c8-a9f2-d58ad1596eae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s\",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \",\n",
        "    \"tom is\", \"tom s\",\n",
        "    \"what s\", \"what a\",\n",
        "   \"are you\", \"do you\",\n",
        "   \"what is\", \"tom was\",\n",
        "   \"don t\", \"it s\", \"where s\",\n",
        "   \"where did\", \"where is\",\n",
        ")\n",
        "\n",
        "def should_keep_row(row):\n",
        "    \"\"\" Should the current row be kept as training set\"\"\"\n",
        "    # indo_num_words = len(word_tokenize(row[\"Indonesian\"]))\n",
        "    eng_num_words = len(word_tokenize(row[\"English\"]))\n",
        "    max_words_required = MAX_LENGTH - 2\n",
        "\n",
        "    return eng_num_words <= max_words_required\n",
        "\n",
        "df[\"keep_row\"] = df.apply(should_keep_row, axis=1)\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "print(\"Current shape: \" + str(df.shape))\n",
        "df = df[df[\"keep_row\"]]\n",
        "print(\"New shape: \" + str(df.shape))\n",
        "df.head()\n",
        "df = df.reset_index().drop(columns=[\"keep_row\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHnwWXPlp-WM",
        "outputId": "27c8bd3f-4392-4ade-dbbf-0770c699c163",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First English sentence: ['<s>', 'run', '!', '</s>']\n",
            "First Indo sentence: ['<s>', 'lari', '!', '</s>']\n",
            "First 10 Indonesian words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '!'), (4, 'lari'), (5, '?'), (6, 'siapa'), (7, 'wow'), (8, 'tolong'), (9, 'lompat')]\n",
            "\n",
            "First 10 English words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '!'), (4, 'run'), (5, '?'), (6, 'who'), (7, 'wow'), (8, 'help'), (9, 'jump')]\n",
            "First 10 Indonesian words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '!'), (4, 'lari'), (5, '?'), (6, 'siapa'), (7, 'wow'), (8, 'tolong'), (9, 'lompat')]\n",
            "\n",
            "First 10 English words in Dictionary:\n",
            " [(0, '<s>'), (1, '</s>'), (2, 'UNK'), (3, '!'), (4, 'run'), (5, '?'), (6, 'who'), (7, 'wow'), (8, 'help'), (9, 'jump')]\n"
          ]
        }
      ],
      "source": [
        "# Use a unique string to indicate START and END of a sentence.\n",
        "# Assign a unique index to them.\n",
        "START, START_IDX = '<s>',  0\n",
        "END, END_IDX = '</s>', 1\n",
        "UNK, UNK_IDX = 'UNK', 2\n",
        "\n",
        "SOS_token = START_IDX\n",
        "EOS_token = END_IDX\n",
        "\n",
        "english_sents = df['English'].apply(str.lower).apply(word_tokenize)\n",
        "english_sents = english_sents.map(lambda x: [START] + x + [END])\n",
        "\n",
        "indo_sents = df['Indonesian'].apply(str.lower).apply(word_tokenize)\n",
        "indo_sents = indo_sents.map(lambda x: [START] + x + [END])\n",
        "\n",
        "print('First English sentence:', english_sents[0])\n",
        "print('First Indo sentence:', indo_sents[0])\n",
        "\n",
        "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
        "english_vocab.add_documents(english_sents)\n",
        "\n",
        "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
        "indo_vocab.add_documents(indo_sents)\n",
        "\n",
        "# First ten words in the vocabulary.\n",
        "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
        "print()\n",
        "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])\n",
        "\n",
        "english_vocab = Dictionary([['<s>'], ['</s>'],['UNK']])\n",
        "english_vocab.add_documents(english_sents)\n",
        "\n",
        "indo_vocab = Dictionary([['<s>'], ['</s>'], ['UNK']])\n",
        "indo_vocab.add_documents(indo_sents)\n",
        "\n",
        "# First ten words in the vocabulary.\n",
        "print('First 10 Indonesian words in Dictionary:\\n', sorted(indo_vocab.items())[:10])\n",
        "print()\n",
        "print('First 10 English words in Dictionary:\\n', sorted(english_vocab.items())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czemQI48p-WO"
      },
      "source": [
        "## BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9a9U3uOlp-WP"
      },
      "outputs": [],
      "source": [
        "#input val_sent_pairs[0] english input to translate output is candidate\n",
        "#val_sent_pairs[1] reference\n",
        "def calculate_bleu_score(reference_sent,candidate_sent):\n",
        "    reference = [word_tokenize(reference_sent)]\n",
        "    candidate = word_tokenize(candidate_sent)\n",
        "\n",
        "    if '<s>' in candidate:\n",
        "        candidate.remove('<s>')\n",
        "    if '</s>' in candidate:\n",
        "        candidate.remove('</s>')\n",
        "    gram_1_score = sentence_bleu(reference,candidate,weights=(1, 0, 0, 0))\n",
        "    gram_2_score = sentence_bleu(reference,candidate,weights=(0.5, 0.5, 0, 0))\n",
        "    gram_3_score = sentence_bleu(reference,candidate,weights=(0.33, 0.33, 0.33, 0))\n",
        "    gram_4_score = sentence_bleu(reference,candidate,weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    blue_score = (gram_1_score+gram_2_score+gram_3_score+gram_4_score)/4\n",
        "    #print(blue_score)\n",
        "    return blue_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HntAoFqGp-WW",
        "outputId": "58f3bb99-2540-49dd-bb78-231ae765469a",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0],\n",
              "        [110],\n",
              "        [ 23],\n",
              "        [129],\n",
              "        [  5],\n",
              "        [  1]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Vectorizes a sentence with a given vocab\n",
        "def vectorize_sent(sent, vocab):\n",
        "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END], unknown_word_index=2)\n",
        "\n",
        "# Creates a PyTorch variable from a sentence against a given vocab\n",
        "def variable_from_sent(sent, vocab):\n",
        "    vsent = vectorize_sent(sent, vocab)\n",
        "    #print(vsent)\n",
        "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
        "    #print(result)\n",
        "    return result.cuda() if use_cuda else result\n",
        "\n",
        "# Test\n",
        "new_kopi = \"Is it love?\"\n",
        "variable_from_sent(new_kopi, english_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJsfM5MRp-WZ",
        "pycharm": {}
      },
      "source": [
        "## Split into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FHDR32sp-WZ",
        "outputId": "5f0ec7a4-ece9-4e55-b8d5-d1896321731d",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13197, 3)\n",
            "(2329, 3)\n",
            "billy sangat tua .\n",
            "('billy is very old .', 'billy sangat tua .')\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_val = train_test_split(df, test_size=0.15)\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "\n",
        "df_train = df_train.reset_index(drop=True)\n",
        "df_val = df_val.reset_index(drop=True)\n",
        "df_train.head()\n",
        "\n",
        "indo_tensors = df_train['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
        "print(df_train.iloc[0]['Indonesian'])\n",
        "df_train\n",
        "\n",
        "english_tensors = df_train['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
        "#print(df_train.iloc[0]['English'])\n",
        "#print(english_tensors[0])\n",
        "# Now, each item in `sent_pairs` is our data point.\n",
        "#print(\"############################\")\n",
        "sent_pairs = list(zip(english_tensors.values, indo_tensors.values))\n",
        "#print(sent_pairs[:5])\n",
        "#print(\"############################\")\n",
        "pairs = list(zip(df_train['English'], df_train['Indonesian']))\n",
        "print(pairs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z5n6tFjp-Wc",
        "outputId": "94569fea-6269-467a-914f-8181a57df0ab",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('i mean it .', 'aku serius .')\n"
          ]
        }
      ],
      "source": [
        "def get_validation_pairs(df_val_in): #MOD Anurag\n",
        "    indo_val_tensors = df_val_in['Indonesian'].apply(lambda s: variable_from_sent(s, indo_vocab))\n",
        "    english_val_tensors = df_val_in['English'].apply(lambda s: variable_from_sent(s, english_vocab))\n",
        "    val_sent_tensor_pairs = list(zip(english_val_tensors.values, indo_val_tensors.values))\n",
        "    val_sent_pairs = list(zip(df_val_in['English'], df_val_in['Indonesian']))\n",
        "    return val_sent_pairs, val_sent_tensor_pairs\n",
        "\n",
        "\n",
        "val_sent_pairs, val_sent_tensor_pairs = get_validation_pairs(df_val) #MOD Anurag\n",
        "print(val_sent_pairs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5roJrm8p-Wf",
        "outputId": "82738cc7-740b-4d82-f1f3-d47581d3a9c6",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('this is nonsense .', 'ini omong kosong .')\n"
          ]
        }
      ],
      "source": [
        "print(val_sent_pairs[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apAoSCbup-Wh",
        "outputId": "6f4d4dc7-45ca-48e8-c46b-4d9337505df2",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('tom is david s uncle .', 'tom adalah paman david .')\n"
          ]
        }
      ],
      "source": [
        "print(val_sent_pairs[154])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhK0Iqjzp-Wj",
        "outputId": "c290fefd-ac82-4d8f-bfdc-657142367f60",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tom is david s uncle .\n",
            "[49]\n",
            "[110]\n",
            "[3493]\n",
            "[41]\n",
            "[884]\n",
            "[10]\n"
          ]
        }
      ],
      "source": [
        "print(val_sent_pairs[154][0])\n",
        "\n",
        "for w in val_sent_pairs[154][0].split(' '):\n",
        "    print(english_vocab.doc2idx([w]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIMz1Kzfp-Wl",
        "pycharm": {}
      },
      "source": [
        "## Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DLZyDHRZp-Wm",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BYgy7OPp-Wo",
        "pycharm": {}
      },
      "source": [
        "## Get training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "aisQ_LGVp-Wp",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    #print(\"Train\")\n",
        "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
        "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "def get_validation_loss(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    #print(\"Validation\")\n",
        "    #print(\"Input tensor shape: {0}\".format(input_tensor.shape))\n",
        "    #print(\"Target tensor shape: {0}\".format(target_tensor.shape))\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss = criterion(decoder_output, target_tensor[di])\n",
        "            total_loss += float(loss.item())\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    return total_loss / target_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXA4vX8dp-Wr",
        "pycharm": {}
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cAxSFCEXp-Ws",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "SAVE_PATH = 'results/vanila_seq2seq'\n",
        "\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "  os.makedirs(SAVE_PATH)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points, filename): # pier mod\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.savefig(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YbjQrQPPp-Wv"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_tensor = variable_from_sent(sentence, english_vocab)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('</s>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(indo_vocab.id2token[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nXUXH83fp-Wy"
      },
      "outputs": [],
      "source": [
        "def translate(input_sentence, enc, dec):\n",
        "    output_words = evaluate(enc, dec, input_sentence)\n",
        "#     print('input =', input_sentence)\n",
        "#     print('output =', ' '.join(output_words))\n",
        "    candidate = ' '.join(output_words)\n",
        "    return candidate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qNoZvZHp-W0",
        "pycharm": {}
      },
      "source": [
        "## Training loop and get evaluation result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DG9pppqwp-W1",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, batch_size=1, print_every=1000, save_every=1000, plot_every=100,\n",
        "               learning_rate=0.0001):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    val_losses = []\n",
        "    bleu_scores = []\n",
        "\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    # training_pairs = [sent_pairs[i] for i in range(n_iters)]\n",
        "    training_pairs = [random.sample(sent_pairs, batch_size) for i in range(n_iters)]\n",
        "\n",
        "    # training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "    MAX_PATIENCE = 50\n",
        "    patience = MAX_PATIENCE\n",
        "    prev_val_loss =lowest_so_far = prev_bleu =  999\n",
        "    highest_so_far = -np.inf # for bleu\n",
        "    stopping_criteria_on = True\n",
        "    using_bleu_stopping = False\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        # print(\"################################\")\n",
        "        # print(training_pair)\n",
        "        input_tensor = training_pair[0][0]\n",
        "        target_tensor = training_pair[0][1]\n",
        "        # print(\"printing tensors for training...\")\n",
        "        # print(input_tensor)\n",
        "        # print(target_tensor)\n",
        "\n",
        "        loss = get_train_loss(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                              criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        stopping_delta = 0.001  # if improvement is not more than this amount after n tries, exit the loop\n",
        "\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('Training loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                                        iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "            total_val_loss = 0\n",
        "            total_bleu_score = 0\n",
        "            total_val_pairs = len(val_sent_tensor_pairs)\n",
        "\n",
        "            for itr in range(0, len(val_sent_tensor_pairs)):\n",
        "                val_input_tensor = val_sent_tensor_pairs[itr][0]\n",
        "                val_target_tensor = val_sent_tensor_pairs[itr][1]\n",
        "                # print(\"Validation record: {0}\".format(itr))\n",
        "                # print(val_sent_pairs[itr])\n",
        "                #calc blue score\n",
        "                reference_sent = val_sent_pairs[itr][1]\n",
        "                candidate_sent = translate(val_sent_pairs[itr][0], encoder, decoder)\n",
        "                bleu_score = calculate_bleu_score(reference_sent,candidate_sent)\n",
        "                total_bleu_score += bleu_score\n",
        "                val_loss = get_validation_loss(val_input_tensor, val_target_tensor, encoder, decoder, criterion)\n",
        "                total_val_loss += val_loss\n",
        "\n",
        "            avg_val_loss = total_val_loss / total_val_pairs\n",
        "            val_losses.append(avg_val_loss)\n",
        "            avg_bleu_scores = total_bleu_score / total_val_pairs\n",
        "            bleu_scores.append(avg_bleu_scores)\n",
        "\n",
        "            print('Validation loss: %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                                          iter, iter / n_iters * 100, avg_val_loss))\n",
        "            print('Bleu scores: %s (%d %d%%) %.8f' % (timeSince(start, iter / n_iters),\n",
        "                                                          iter, iter / n_iters * 100, avg_bleu_scores))\n",
        "            if  stopping_criteria_on:\n",
        "                if not using_bleu_stopping:\n",
        "                    if (prev_val_loss - avg_val_loss) > stopping_delta and avg_val_loss < lowest_so_far:\n",
        "                        print(f\"Improvement in validation loss, saving model. Prev {prev_val_loss} Curr {avg_val_loss}\")\n",
        "                        lowest_so_far = avg_val_loss\n",
        "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
        "                        print('save encoder weights to ', encoder_save_path)\n",
        "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
        "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
        "                        print('save decoder weights to ', decoder_save_path)\n",
        "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
        "                        patience = MAX_PATIENCE # reset to max\n",
        "                    else:\n",
        "                        print(f\"No improvement in validation loss, losing patience {patience}\")\n",
        "                        patience -= 1\n",
        "\n",
        "                    if patience == 0:  # break out of training\n",
        "                        break\n",
        "\n",
        "                    prev_val_loss = avg_val_loss\n",
        "                else: # bleu\n",
        "                    if (avg_bleu_scores - prev_bleu) > stopping_delta and avg_bleu_scores > highest_so_far:\n",
        "                        print(f\"Improvement in bleu scores, saving model. Prev {prev_bleu} Curr {avg_bleu_scores}\")\n",
        "                        highest_so_far = avg_bleu_scores\n",
        "                        encoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_encoder')\n",
        "                        print('save encoder weights to ', encoder_save_path)\n",
        "                        torch.save(encoder.state_dict(), encoder_save_path)\n",
        "                        decoder_save_path = '%s/%s.pth' % (SAVE_PATH, 'best_decoder')\n",
        "                        print('save decoder weights to ', decoder_save_path)\n",
        "                        torch.save(decoder.state_dict(), decoder_save_path)\n",
        "                        patience = MAX_PATIENCE # reset to max\n",
        "                    else:\n",
        "                        print(f\"No improvement in bleu scores, losing patience {patience}\")\n",
        "                        patience -= 1\n",
        "\n",
        "                    if patience == 0:  # break out of training\n",
        "                        break\n",
        "\n",
        "                    prev_bleu = avg_bleu_scores\n",
        "\n",
        "\n",
        "            print(\"##########################################################\")\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "        # # save trained encoder and decoder\n",
        "        # if iter % save_every == 0:\n",
        "        #     encoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'encoder', iter)\n",
        "        #     print('save encoder weights to ', encoder_save_path)\n",
        "        #     torch.save(encoder.state_dict(), encoder_save_path)\n",
        "        #     decoder_save_path = '%s/%s-%d.pth' % (SAVE_PATH, 'decoder', iter)\n",
        "        #     print('save decoder weights to ', decoder_save_path)\n",
        "        #     torch.save(decoder.state_dict(), decoder_save_path)\n",
        "\n",
        "    showPlot(plot_losses, 'train_plot.png')\n",
        "    showPlot(val_losses, 'validation_plot.png')\n",
        "    showPlot(bleu_scores,'bleu_scores_plot.png')\n",
        "    return plot_losses, val_losses, bleu_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bJ3YTGgZp-W3"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRio-rM_p-W5",
        "pycharm": {}
      },
      "source": [
        "## Perform training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRZp2jlYp-W5",
        "outputId": "f8d2f120-4bb6-41ed-88bf-6191f543fbf0",
        "pycharm": {}
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0m 24s (- 30m 0s) (1000 1%) 4.0366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0m 55s (- 68m 6s) (1000 1%) 3.8225\n",
            "Bleu scores: 0m 55s (- 68m 6s) (1000 1%) 0.03272615\n",
            "Improvement in validation loss, saving model. Prev 999 Curr 3.8224614147989007\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 1m 13s (- 44m 32s) (2000 2%) 3.9751\n",
            "Validation loss: 1m 43s (- 63m 7s) (2000 2%) 3.8544\n",
            "Bleu scores: 1m 43s (- 63m 7s) (2000 2%) 0.03560100\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 2m 3s (- 49m 35s) (3000 4%) 3.9345\n",
            "Validation loss: 2m 35s (- 62m 1s) (3000 4%) 3.7927\n",
            "Bleu scores: 2m 35s (- 62m 1s) (3000 4%) 0.04109130\n",
            "Improvement in validation loss, saving model. Prev 3.85435349402071 Curr 3.792748601424793\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 2m 53s (- 51m 15s) (4000 5%) 3.7988\n",
            "Validation loss: 3m 26s (- 61m 8s) (4000 5%) 3.9153\n",
            "Bleu scores: 3m 26s (- 61m 8s) (4000 5%) 0.04051194\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 3m 44s (- 52m 29s) (5000 6%) 3.7950\n",
            "Validation loss: 4m 18s (- 60m 14s) (5000 6%) 3.8263\n",
            "Bleu scores: 4m 18s (- 60m 15s) (5000 6%) 0.03922969\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "Training loss: 4m 36s (- 52m 57s) (6000 8%) 3.6256\n",
            "Validation loss: 5m 8s (- 59m 7s) (6000 8%) 3.6709\n",
            "Bleu scores: 5m 8s (- 59m 7s) (6000 8%) 0.04431257\n",
            "Improvement in validation loss, saving model. Prev 3.8263435348518615 Curr 3.6709464735761776\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 5m 27s (- 53m 2s) (7000 9%) 3.5115\n",
            "Validation loss: 6m 1s (- 58m 28s) (7000 9%) 3.6454\n",
            "Bleu scores: 6m 1s (- 58m 28s) (7000 9%) 0.04846575\n",
            "Improvement in validation loss, saving model. Prev 3.6709464735761776 Curr 3.645384299514159\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 6m 20s (- 53m 3s) (8000 10%) 3.4014\n",
            "Validation loss: 6m 52s (- 57m 37s) (8000 10%) 3.6043\n",
            "Bleu scores: 6m 52s (- 57m 37s) (8000 10%) 0.05571514\n",
            "Improvement in validation loss, saving model. Prev 3.645384299514159 Curr 3.6043008936674754\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 7m 12s (- 52m 48s) (9000 12%) 3.3789\n",
            "Validation loss: 7m 45s (- 56m 54s) (9000 12%) 3.5115\n",
            "Bleu scores: 7m 45s (- 56m 54s) (9000 12%) 0.05711487\n",
            "Improvement in validation loss, saving model. Prev 3.6043008936674754 Curr 3.5115118627394217\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 8m 4s (- 52m 30s) (10000 13%) 3.3044\n",
            "Validation loss: 8m 38s (- 56m 7s) (10000 13%) 3.4401\n",
            "Bleu scores: 8m 38s (- 56m 7s) (10000 13%) 0.06256034\n",
            "Improvement in validation loss, saving model. Prev 3.5115118627394217 Curr 3.4400862333463658\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 8m 56s (- 52m 2s) (11000 14%) 3.2226\n",
            "Validation loss: 9m 32s (- 55m 28s) (11000 14%) 3.4231\n",
            "Bleu scores: 9m 32s (- 55m 28s) (11000 14%) 0.06493094\n",
            "Improvement in validation loss, saving model. Prev 3.4400862333463658 Curr 3.423087566344845\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 9m 50s (- 51m 41s) (12000 16%) 3.0467\n",
            "Validation loss: 10m 24s (- 54m 41s) (12000 16%) 3.3189\n",
            "Bleu scores: 10m 24s (- 54m 41s) (12000 16%) 0.07045213\n",
            "Improvement in validation loss, saving model. Prev 3.423087566344845 Curr 3.3188735574174917\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 10m 45s (- 51m 18s) (13000 17%) 2.9779\n",
            "Validation loss: 11m 20s (- 54m 4s) (13000 17%) 3.3227\n",
            "Bleu scores: 11m 20s (- 54m 4s) (13000 17%) 0.07112744\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 11m 39s (- 50m 48s) (14000 18%) 2.9168\n",
            "Validation loss: 12m 12s (- 53m 10s) (14000 18%) 3.1807\n",
            "Bleu scores: 12m 12s (- 53m 10s) (14000 18%) 0.07759654\n",
            "Improvement in validation loss, saving model. Prev 3.322674406757477 Curr 3.180681559201748\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 12m 30s (- 50m 3s) (15000 20%) 2.7666\n",
            "Validation loss: 13m 5s (- 52m 20s) (15000 20%) 3.1173\n",
            "Bleu scores: 13m 5s (- 52m 20s) (15000 20%) 0.07891112\n",
            "Improvement in validation loss, saving model. Prev 3.180681559201748 Curr 3.117325234474176\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 13m 23s (- 49m 23s) (16000 21%) 2.7434\n",
            "Validation loss: 13m 57s (- 51m 29s) (16000 21%) 3.0661\n",
            "Bleu scores: 13m 57s (- 51m 29s) (16000 21%) 0.08316722\n",
            "Improvement in validation loss, saving model. Prev 3.117325234474176 Curr 3.066138411488043\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 14m 16s (- 48m 41s) (17000 22%) 2.6790\n",
            "Validation loss: 14m 49s (- 50m 35s) (17000 22%) 3.0038\n",
            "Bleu scores: 14m 49s (- 50m 35s) (17000 22%) 0.08792973\n",
            "Improvement in validation loss, saving model. Prev 3.066138411488043 Curr 3.003813466014417\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 15m 8s (- 47m 58s) (18000 24%) 2.5824\n",
            "Validation loss: 15m 43s (- 49m 48s) (18000 24%) 3.0673\n",
            "Bleu scores: 15m 43s (- 49m 48s) (18000 24%) 0.09075157\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 16m 2s (- 47m 17s) (19000 25%) 2.5552\n",
            "Validation loss: 16m 37s (- 48m 58s) (19000 25%) 2.8816\n",
            "Bleu scores: 16m 37s (- 48m 58s) (19000 25%) 0.09515171\n",
            "Improvement in validation loss, saving model. Prev 3.067323515305684 Curr 2.881588836661988\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 16m 55s (- 46m 33s) (20000 26%) 2.5421\n",
            "Validation loss: 17m 29s (- 48m 5s) (20000 26%) 2.7912\n",
            "Bleu scores: 17m 29s (- 48m 5s) (20000 26%) 0.10252253\n",
            "Improvement in validation loss, saving model. Prev 2.881588836661988 Curr 2.791222103658615\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 17m 47s (- 45m 46s) (21000 28%) 2.3669\n",
            "Validation loss: 18m 21s (- 47m 13s) (21000 28%) 2.7657\n",
            "Bleu scores: 18m 21s (- 47m 13s) (21000 28%) 0.10666202\n",
            "Improvement in validation loss, saving model. Prev 2.791222103658615 Curr 2.7657004695037606\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 18m 41s (- 45m 1s) (22000 29%) 2.3997\n",
            "Validation loss: 19m 14s (- 46m 20s) (22000 29%) 2.7054\n",
            "Bleu scores: 19m 14s (- 46m 20s) (22000 29%) 0.10966275\n",
            "Improvement in validation loss, saving model. Prev 2.7657004695037606 Curr 2.7054428775776054\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 19m 33s (- 44m 12s) (23000 30%) 2.2532\n",
            "Validation loss: 20m 6s (- 45m 28s) (23000 30%) 2.6669\n",
            "Bleu scores: 20m 6s (- 45m 28s) (23000 30%) 0.11784735\n",
            "Improvement in validation loss, saving model. Prev 2.7054428775776054 Curr 2.666852104897487\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 20m 25s (- 43m 25s) (24000 32%) 2.2321\n",
            "Validation loss: 21m 0s (- 44m 38s) (24000 32%) 2.8091\n",
            "Bleu scores: 21m 0s (- 44m 38s) (24000 32%) 0.11751092\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 21m 19s (- 42m 38s) (25000 33%) 2.1701\n",
            "Validation loss: 21m 52s (- 43m 44s) (25000 33%) 2.5694\n",
            "Bleu scores: 21m 52s (- 43m 44s) (25000 33%) 0.12694022\n",
            "Improvement in validation loss, saving model. Prev 2.809116724627479 Curr 2.569427985914164\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 22m 12s (- 41m 50s) (26000 34%) 2.0964\n",
            "Validation loss: 22m 45s (- 42m 52s) (26000 34%) 2.5141\n",
            "Bleu scores: 22m 45s (- 42m 52s) (26000 34%) 0.13442463\n",
            "Improvement in validation loss, saving model. Prev 2.569427985914164 Curr 2.514109862564909\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 23m 4s (- 41m 1s) (27000 36%) 1.9768\n",
            "Validation loss: 23m 38s (- 42m 1s) (27000 36%) 2.4511\n",
            "Bleu scores: 23m 38s (- 42m 1s) (27000 36%) 0.13835035\n",
            "Improvement in validation loss, saving model. Prev 2.514109862564909 Curr 2.4510588582527664\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 23m 57s (- 40m 12s) (28000 37%) 1.9274\n",
            "Validation loss: 24m 31s (- 41m 9s) (28000 37%) 2.4254\n",
            "Bleu scores: 24m 31s (- 41m 9s) (28000 37%) 0.15023428\n",
            "Improvement in validation loss, saving model. Prev 2.4510588582527664 Curr 2.4253795935584996\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 24m 50s (- 39m 24s) (29000 38%) 1.9058\n",
            "Validation loss: 25m 24s (- 40m 18s) (29000 38%) 2.3866\n",
            "Bleu scores: 25m 24s (- 40m 18s) (29000 38%) 0.15345635\n",
            "Improvement in validation loss, saving model. Prev 2.4253795935584996 Curr 2.386608799700348\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 25m 43s (- 38m 35s) (30000 40%) 1.8472\n",
            "Validation loss: 26m 16s (- 39m 25s) (30000 40%) 2.3069\n",
            "Bleu scores: 26m 16s (- 39m 25s) (30000 40%) 0.16178298\n",
            "Improvement in validation loss, saving model. Prev 2.386608799700348 Curr 2.3069164610011836\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 26m 35s (- 37m 44s) (31000 41%) 1.7821\n",
            "Validation loss: 27m 9s (- 38m 32s) (31000 41%) 2.2556\n",
            "Bleu scores: 27m 9s (- 38m 32s) (31000 41%) 0.17228220\n",
            "Improvement in validation loss, saving model. Prev 2.3069164610011836 Curr 2.255614977998719\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 27m 28s (- 36m 54s) (32000 42%) 1.7207\n",
            "Validation loss: 28m 2s (- 37m 40s) (32000 42%) 2.2223\n",
            "Bleu scores: 28m 2s (- 37m 40s) (32000 42%) 0.17930579\n",
            "Improvement in validation loss, saving model. Prev 2.255614977998719 Curr 2.2223240600428436\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 28m 20s (- 36m 4s) (33000 44%) 1.7450\n",
            "Validation loss: 28m 56s (- 36m 49s) (33000 44%) 2.2023\n",
            "Bleu scores: 28m 56s (- 36m 49s) (33000 44%) 0.18143850\n",
            "Improvement in validation loss, saving model. Prev 2.2223240600428436 Curr 2.202294312243174\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 29m 14s (- 35m 16s) (34000 45%) 1.6289\n",
            "Validation loss: 29m 48s (- 35m 57s) (34000 45%) 2.1539\n",
            "Bleu scores: 29m 48s (- 35m 57s) (34000 45%) 0.19260311\n",
            "Improvement in validation loss, saving model. Prev 2.202294312243174 Curr 2.1539061137251334\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 30m 7s (- 34m 25s) (35000 46%) 1.5999\n",
            "Validation loss: 30m 41s (- 35m 4s) (35000 46%) 2.1103\n",
            "Bleu scores: 30m 41s (- 35m 4s) (35000 46%) 0.19564112\n",
            "Improvement in validation loss, saving model. Prev 2.1539061137251334 Curr 2.1103498558927174\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 30m 59s (- 33m 34s) (36000 48%) 1.5608\n",
            "Validation loss: 31m 33s (- 34m 11s) (36000 48%) 2.0495\n",
            "Bleu scores: 31m 33s (- 34m 11s) (36000 48%) 0.20847611\n",
            "Improvement in validation loss, saving model. Prev 2.1103498558927174 Curr 2.049539957403828\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 31m 52s (- 32m 44s) (37000 49%) 1.4310\n",
            "Validation loss: 32m 26s (- 33m 18s) (37000 49%) 2.0404\n",
            "Bleu scores: 32m 26s (- 33m 18s) (37000 49%) 0.20850458\n",
            "Improvement in validation loss, saving model. Prev 2.049539957403828 Curr 2.0403513005896445\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 32m 45s (- 31m 53s) (38000 50%) 1.4587\n",
            "Validation loss: 33m 18s (- 32m 26s) (38000 50%) 1.9936\n",
            "Bleu scores: 33m 18s (- 32m 26s) (38000 50%) 0.22175030\n",
            "Improvement in validation loss, saving model. Prev 2.0403513005896445 Curr 1.9935802228897666\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 33m 37s (- 31m 2s) (39000 52%) 1.5330\n",
            "Validation loss: 34m 11s (- 31m 34s) (39000 52%) 1.9418\n",
            "Bleu scores: 34m 11s (- 31m 34s) (39000 52%) 0.22576731\n",
            "Improvement in validation loss, saving model. Prev 1.9935802228897666 Curr 1.9418355158041727\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 34m 30s (- 30m 12s) (40000 53%) 1.3853\n",
            "Validation loss: 35m 3s (- 30m 40s) (40000 53%) 1.8863\n",
            "Bleu scores: 35m 3s (- 30m 40s) (40000 53%) 0.23466829\n",
            "Improvement in validation loss, saving model. Prev 1.9418355158041727 Curr 1.8863220456689564\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 35m 23s (- 29m 20s) (41000 54%) 1.2989\n",
            "Validation loss: 35m 57s (- 29m 49s) (41000 54%) 1.8571\n",
            "Bleu scores: 35m 57s (- 29m 49s) (41000 54%) 0.23992299\n",
            "Improvement in validation loss, saving model. Prev 1.8863220456689564 Curr 1.8570696483937197\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 36m 17s (- 28m 30s) (42000 56%) 1.3185\n",
            "Validation loss: 36m 49s (- 28m 56s) (42000 56%) 1.8176\n",
            "Bleu scores: 36m 49s (- 28m 56s) (42000 56%) 0.24791971\n",
            "Improvement in validation loss, saving model. Prev 1.8570696483937197 Curr 1.8175860204478786\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 37m 8s (- 27m 38s) (43000 57%) 1.2067\n",
            "Validation loss: 37m 42s (- 28m 3s) (43000 57%) 1.8113\n",
            "Bleu scores: 37m 42s (- 28m 3s) (43000 57%) 0.25321544\n",
            "Improvement in validation loss, saving model. Prev 1.8175860204478786 Curr 1.8112735999406826\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 38m 1s (- 26m 47s) (44000 58%) 1.2603\n",
            "Validation loss: 38m 35s (- 27m 11s) (44000 58%) 1.8017\n",
            "Bleu scores: 38m 35s (- 27m 11s) (44000 58%) 0.25570817\n",
            "Improvement in validation loss, saving model. Prev 1.8112735999406826 Curr 1.8016933864286584\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 38m 54s (- 25m 56s) (45000 60%) 1.1643\n",
            "Validation loss: 39m 28s (- 26m 19s) (45000 60%) 1.7883\n",
            "Bleu scores: 39m 28s (- 26m 19s) (45000 60%) 0.26463202\n",
            "Improvement in validation loss, saving model. Prev 1.8016933864286584 Curr 1.7882636306826438\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 39m 47s (- 25m 5s) (46000 61%) 1.1308\n",
            "Validation loss: 40m 21s (- 25m 26s) (46000 61%) 1.7419\n",
            "Bleu scores: 40m 21s (- 25m 26s) (46000 61%) 0.26959331\n",
            "Improvement in validation loss, saving model. Prev 1.7882636306826438 Curr 1.7419093999441637\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 40m 40s (- 24m 14s) (47000 62%) 1.1629\n",
            "Validation loss: 41m 14s (- 24m 34s) (47000 62%) 1.6947\n",
            "Bleu scores: 41m 14s (- 24m 34s) (47000 62%) 0.27375520\n",
            "Improvement in validation loss, saving model. Prev 1.7419093999441637 Curr 1.6947486990317675\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 41m 33s (- 23m 22s) (48000 64%) 1.1253\n",
            "Validation loss: 42m 7s (- 23m 41s) (48000 64%) 1.6889\n",
            "Bleu scores: 42m 7s (- 23m 41s) (48000 64%) 0.27650532\n",
            "Improvement in validation loss, saving model. Prev 1.6947486990317675 Curr 1.6888964340288075\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 42m 26s (- 22m 31s) (49000 65%) 1.0901\n",
            "Validation loss: 43m 2s (- 22m 50s) (49000 65%) 1.6375\n",
            "Bleu scores: 43m 2s (- 22m 50s) (49000 65%) 0.28041436\n",
            "Improvement in validation loss, saving model. Prev 1.6888964340288075 Curr 1.6375191875988953\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 43m 20s (- 21m 40s) (50000 66%) 1.1026\n",
            "Validation loss: 43m 55s (- 21m 57s) (50000 66%) 1.6329\n",
            "Bleu scores: 43m 55s (- 21m 57s) (50000 66%) 0.28449856\n",
            "Improvement in validation loss, saving model. Prev 1.6375191875988953 Curr 1.6328520590705495\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 44m 14s (- 20m 49s) (51000 68%) 1.0060\n",
            "Validation loss: 44m 49s (- 21m 5s) (51000 68%) 1.6084\n",
            "Bleu scores: 44m 49s (- 21m 5s) (51000 68%) 0.28828412\n",
            "Improvement in validation loss, saving model. Prev 1.6328520590705495 Curr 1.6084194796204097\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 45m 8s (- 19m 57s) (52000 69%) 1.0172\n",
            "Validation loss: 45m 42s (- 20m 13s) (52000 69%) 1.5904\n",
            "Bleu scores: 45m 42s (- 20m 13s) (52000 69%) 0.29087468\n",
            "Improvement in validation loss, saving model. Prev 1.6084194796204097 Curr 1.5904199206846563\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 46m 1s (- 19m 6s) (53000 70%) 0.9922\n",
            "Validation loss: 46m 36s (- 19m 20s) (53000 70%) 1.5748\n",
            "Bleu scores: 46m 36s (- 19m 20s) (53000 70%) 0.29764863\n",
            "Improvement in validation loss, saving model. Prev 1.5904199206846563 Curr 1.5748039133349259\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 46m 55s (- 18m 14s) (54000 72%) 1.0098\n",
            "Validation loss: 47m 29s (- 18m 28s) (54000 72%) 1.5433\n",
            "Bleu scores: 47m 29s (- 18m 28s) (54000 72%) 0.30132999\n",
            "Improvement in validation loss, saving model. Prev 1.5748039133349259 Curr 1.5432982946157539\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 47m 48s (- 17m 22s) (55000 73%) 0.9880\n",
            "Validation loss: 48m 22s (- 17m 35s) (55000 73%) 1.5317\n",
            "Bleu scores: 48m 22s (- 17m 35s) (55000 73%) 0.30413984\n",
            "Improvement in validation loss, saving model. Prev 1.5432982946157539 Curr 1.5316578830309617\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 48m 41s (- 16m 31s) (56000 74%) 0.9983\n",
            "Validation loss: 49m 15s (- 16m 42s) (56000 74%) 1.5130\n",
            "Bleu scores: 49m 15s (- 16m 42s) (56000 74%) 0.30746495\n",
            "Improvement in validation loss, saving model. Prev 1.5316578830309617 Curr 1.512967362070375\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 49m 35s (- 15m 39s) (57000 76%) 0.9434\n",
            "Validation loss: 50m 8s (- 15m 50s) (57000 76%) 1.5143\n",
            "Bleu scores: 50m 8s (- 15m 50s) (57000 76%) 0.30817530\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 50m 28s (- 14m 47s) (58000 77%) 0.8898\n",
            "Validation loss: 51m 2s (- 14m 57s) (58000 77%) 1.4846\n",
            "Bleu scores: 51m 2s (- 14m 57s) (58000 77%) 0.31024462\n",
            "Improvement in validation loss, saving model. Prev 1.514252044651435 Curr 1.4846458685723973\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 51m 20s (- 13m 55s) (59000 78%) 0.8614\n",
            "Validation loss: 51m 54s (- 14m 4s) (59000 78%) 1.4701\n",
            "Bleu scores: 51m 54s (- 14m 4s) (59000 78%) 0.31183762\n",
            "Improvement in validation loss, saving model. Prev 1.4846458685723973 Curr 1.4701474588717038\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 52m 14s (- 13m 3s) (60000 80%) 0.9173\n",
            "Validation loss: 52m 47s (- 13m 11s) (60000 80%) 1.4658\n",
            "Bleu scores: 52m 47s (- 13m 11s) (60000 80%) 0.31103133\n",
            "Improvement in validation loss, saving model. Prev 1.4701474588717038 Curr 1.4658023749981064\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 53m 6s (- 12m 11s) (61000 81%) 0.8393\n",
            "Validation loss: 53m 40s (- 12m 19s) (61000 81%) 1.4737\n",
            "Bleu scores: 53m 40s (- 12m 19s) (61000 81%) 0.31602962\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 53m 59s (- 11m 19s) (62000 82%) 0.7745\n",
            "Validation loss: 54m 32s (- 11m 26s) (62000 82%) 1.4275\n",
            "Bleu scores: 54m 32s (- 11m 26s) (62000 82%) 0.32032374\n",
            "Improvement in validation loss, saving model. Prev 1.473696136352078 Curr 1.4275471043722074\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 54m 52s (- 10m 27s) (63000 84%) 0.8088\n",
            "Validation loss: 55m 25s (- 10m 33s) (63000 84%) 1.4323\n",
            "Bleu scores: 55m 25s (- 10m 33s) (63000 84%) 0.31742950\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 55m 45s (- 9m 34s) (64000 85%) 0.8240\n",
            "Validation loss: 56m 19s (- 9m 40s) (64000 85%) 1.4450\n",
            "Bleu scores: 56m 19s (- 9m 40s) (64000 85%) 0.32081027\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "Training loss: 56m 38s (- 8m 42s) (65000 86%) 0.8255\n",
            "Validation loss: 57m 12s (- 8m 48s) (65000 86%) 1.3962\n",
            "Bleu scores: 57m 12s (- 8m 48s) (65000 86%) 0.32428296\n",
            "Improvement in validation loss, saving model. Prev 1.4450158144046512 Curr 1.396206956114517\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 57m 31s (- 7m 50s) (66000 88%) 0.7775\n",
            "Validation loss: 58m 5s (- 7m 55s) (66000 88%) 1.3985\n",
            "Bleu scores: 58m 5s (- 7m 55s) (66000 88%) 0.32632686\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 58m 24s (- 6m 58s) (67000 89%) 0.8971\n",
            "Validation loss: 58m 58s (- 7m 2s) (67000 89%) 1.3667\n",
            "Bleu scores: 58m 58s (- 7m 2s) (67000 89%) 0.32909879\n",
            "Improvement in validation loss, saving model. Prev 1.398490031737443 Curr 1.3666803704726112\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 59m 17s (- 6m 6s) (68000 90%) 0.7231\n",
            "Validation loss: 59m 51s (- 6m 9s) (68000 90%) 1.3527\n",
            "Bleu scores: 59m 51s (- 6m 9s) (68000 90%) 0.32788513\n",
            "Improvement in validation loss, saving model. Prev 1.3666803704726112 Curr 1.3527472100748983\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 60m 10s (- 5m 13s) (69000 92%) 0.7778\n",
            "Validation loss: 60m 44s (- 5m 16s) (69000 92%) 1.3598\n",
            "Bleu scores: 60m 44s (- 5m 16s) (69000 92%) 0.33250767\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 61m 3s (- 4m 21s) (70000 93%) 0.7425\n",
            "Validation loss: 61m 37s (- 4m 24s) (70000 93%) 1.3359\n",
            "Bleu scores: 61m 37s (- 4m 24s) (70000 93%) 0.33204074\n",
            "Improvement in validation loss, saving model. Prev 1.3598305649436195 Curr 1.3358868086149465\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 61m 56s (- 3m 29s) (71000 94%) 0.7308\n",
            "Validation loss: 62m 30s (- 3m 31s) (71000 94%) 1.3255\n",
            "Bleu scores: 62m 30s (- 3m 31s) (71000 94%) 0.33153735\n",
            "Improvement in validation loss, saving model. Prev 1.3358868086149465 Curr 1.3255482124870706\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 62m 49s (- 2m 37s) (72000 96%) 0.7166\n",
            "Validation loss: 63m 24s (- 2m 38s) (72000 96%) 1.3147\n",
            "Bleu scores: 63m 24s (- 2m 38s) (72000 96%) 0.33646271\n",
            "Improvement in validation loss, saving model. Prev 1.3255482124870706 Curr 1.3147372661534993\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "Training loss: 63m 43s (- 1m 44s) (73000 97%) 0.7295\n",
            "Validation loss: 64m 17s (- 1m 45s) (73000 97%) 1.3240\n",
            "Bleu scores: 64m 17s (- 1m 45s) (73000 97%) 0.33701942\n",
            "No improvement in validation loss, losing patience 50\n",
            "##########################################################\n",
            "Training loss: 64m 36s (- 0m 52s) (74000 98%) 0.7173\n",
            "Validation loss: 65m 15s (- 0m 52s) (74000 98%) 1.3198\n",
            "Bleu scores: 65m 15s (- 0m 52s) (74000 98%) 0.33721424\n",
            "No improvement in validation loss, losing patience 49\n",
            "##########################################################\n",
            "Training loss: 65m 34s (- 0m 0s) (75000 100%) 0.6546\n",
            "Validation loss: 66m 8s (- 0m 0s) (75000 100%) 1.3131\n",
            "Bleu scores: 66m 8s (- 0m 0s) (75000 100%) 0.33662200\n",
            "Improvement in validation loss, saving model. Prev 1.319813140742412 Curr 1.313067531334133\n",
            "save encoder weights to  results/vanila_seq2seq/best_encoder.pth\n",
            "save decoder weights to  results/vanila_seq2seq/best_decoder.pth\n",
            "##########################################################\n",
            "> i promise .\n",
            "= aku janji .\n",
            "< <s> aku bosan . </s>\n",
            "\n",
            "> i m not the only one here who thinks kim isn t a nice guy .\n",
            "= bukan cuma aku saja yang berpikir bahwa kim bukanlah orang yang baik .\n",
            "< <s> aku cuma aku saja yang berpikir bahwa kim adalah orang yang baik . </s>\n",
            "\n",
            "> tom wanted mia to go to boston with him .\n",
            "= tom ingin mia pergi ke boston bersamanya .\n",
            "< <s> tom ingin mia pergi ke boston bersamanya . </s>\n",
            "\n",
            "> michael grudgingly did what mary asked .\n",
            "= michael melakukan apa yang mary minta dengan berat hati .\n",
            "< <s> michael melakukan apa yang mary minta dengan berat hati . </s>\n",
            "\n",
            "> maria hates me .\n",
            "= maria membenciku .\n",
            "< <s> maria membenciku . </s>\n",
            "\n",
            "> i met him by chance at the airport yesterday .\n",
            "= kemarin saya tidak sengaja bertemu dengannya di bandara .\n",
            "< <s> kemarin saya tidak bertemu dengannya di pertemuan . </s>\n",
            "\n",
            "> anything you could do in support of their effort would be very much appreciated .\n",
            "= kami sangat mengharapkan bantuan anda dalam mendukung upaya mereka .\n",
            "< <s> kamu akan berikan dalam sangat dalam . . </s>\n",
            "\n",
            "> i will go to tokyo tomorrow .\n",
            "= aku akan pergi ke tokyo besok .\n",
            "< <s> aku akan pergi ke tokyo besok . </s>\n",
            "\n",
            "> what do you think this means ?\n",
            "= menurutmu apa arti ini ?\n",
            "< <s> apa menurutmu ini ? </s>\n",
            "\n",
            "> maria and mary are here too .\n",
            "= maria dan mary juga ada di sini .\n",
            "< <s> maria dan mary juga ada di sini . </s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 512\n",
        "encoder1 = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, len(indo_vocab)).to(device)\n",
        "\n",
        "plot_losses, val_losses, bleu_scores = trainIters(encoder1, decoder1, 75000, print_every=1000)\n",
        "\n",
        "showPlot(plot_losses, 'train_plot.png')\n",
        "showPlot(val_losses, 'validation_plot.png')\n",
        "showPlot(bleu_scores, 'bleu_plot.png')\n",
        "\n",
        "evaluateRandomly(encoder1, decoder1)\n",
        "\n",
        "output_words = evaluate(encoder1, decoder1, \"do you love me?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jjv9Fiyp-W7",
        "pycharm": {}
      },
      "source": [
        "## Check some translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "F_0qlyXup-W_",
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "loc = ticker.MultipleLocator(base=0.2)\n",
        "ax.yaxis.set_major_locator(loc)\n",
        "plt.title(\"Train and Valid Loss\")\n",
        "plt.plot(plot_losses, label='Train Loss')\n",
        "plt.legend()\n",
        "plt.plot(val_losses, label='Valid Loss')\n",
        "plt.legend()\n",
        "plt.savefig('results.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHkh_erOp-W9",
        "pycharm": {},
        "outputId": "d010e744-ada0-4c44-9cb4-ba1db9d87581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> tom sedang bermain dengan . </s>\n",
            "<s> di sana di sana sana . </s>\n",
            "<s> dia adalah seorang yang </s>\n",
            "<s> dia ingin menjadi . . </s>\n",
            "<s> aku ingin melihat bertemu . </s>\n",
            "<s> hobiku sedang berhembus . . </s>\n",
            "<s> ini sangat murah . </s>\n",
            "<s> tom meminta cara untuk untuk membuat bahasa prancis . </s>\n",
            "<s> tolong duduk di di . . </s>\n",
            "<s> aku berada di di di . . </s>\n"
          ]
        }
      ],
      "source": [
        "print(translate(\"tom is playing with ball .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"she is standing there .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"he is a bad man .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"he wants to sleep .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"i can't see you crying .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"my dog is running around .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"it is very popular .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"she speaks american english to tom's father .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"please eat lunch in the afternoon .\", encoder1, decoder1))\n",
        "\n",
        "print(translate(\"i see red roses in the garden .\", encoder1, decoder1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jLRus0JoGT8Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}